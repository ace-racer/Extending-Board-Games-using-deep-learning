{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product, combinations\n",
    "import math\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anurag/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Conv2D, Lambda, average, Dense, Flatten,MaxPooling2D, BatchNormalization, Dropout, Activation, Subtract, subtract\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (70, 70)\n",
    "\n",
    "def auto_canny(image, sigma=0.33):\n",
    "\t# compute the median of the single channel pixel intensities\n",
    "\tv = np.median(image)\n",
    " \n",
    "\t# apply automatic Canny edge detection using the computed median\n",
    "\tlower = int(max(0, (1.0 - sigma) * v))\n",
    "\tupper = int(min(255, (1.0 + sigma) * v))\n",
    "\tedged = cv2.Canny(image, lower, upper)\n",
    " \n",
    "\t# return the edged image\n",
    "\treturn edged\n",
    "\n",
    "def process_image(image_location):\n",
    "    \"\"\"\n",
    "        Given the image location, process the image\n",
    "    \"\"\"\n",
    "    # print(image_location)\n",
    "    \n",
    "    image = cv2.imread(image_location)\n",
    "    \n",
    "    if image.shape[0] != IMAGE_SIZE[0] or image.shape[1] != IMAGE_SIZE[1]:\n",
    "        # print(\"Resizing the image: {0}\".format(image_location))\n",
    "        resized_image = cv2.resize(image, IMAGE_SIZE, interpolation = cv2.INTER_AREA)\n",
    "    else:\n",
    "        resized_image = image\n",
    "    \n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # get the edges from the image\n",
    "    edges = auto_canny(gray)\n",
    "    #print(edges.shape)\n",
    "    \n",
    "    \n",
    "    # assert(denoised != edges)\n",
    "    # add the two images in a weighted manner\n",
    "    weighted_sum = cv2.addWeighted(gray, 0.8, edges, 0.2, 0)\n",
    "       \n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_locations = {\"b\": [\"bb\", \"wb\"], \"n\": [\"bn\", \"wn\"], \"k\": [\"bk\", \"wk\"], \"p\": [\"bp\", \"wp\"], \"q\": [\"bq\", \"wq\"], \"r\": [\"br\", \"wr\"]}\n",
    "type_name_to_label = { \"p\":0, \"b\":1, \"n\":2, \"r\":3, \"q\": 4, \"k\":5 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below cell need to be replaced with the location of the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(data_path):\n",
    "    X, y = [], []\n",
    "    features_with_labels = []\n",
    "    \n",
    "    for type_name in type_locations:\n",
    "        for folder_name in type_locations[type_name]:\n",
    "            piece_type_folder = os.path.join(data_path, folder_name)\n",
    "            for f in (os.listdir(piece_type_folder)):\n",
    "                if f.endswith(\".jpg\"): \n",
    "                    \n",
    "\n",
    "                    img_file_loc = os.path.join(piece_type_folder, f)\n",
    "                    #print(img_file_loc)\n",
    "                    grayscale_image = process_image(img_file_loc)\n",
    "                    actual_image = grayscale_image\n",
    "                    grayscale_image = grayscale_image[..., np.newaxis]\n",
    "                    label = type_name_to_label[type_name]\n",
    "                    features_with_labels.append({\"feature\": grayscale_image, \"label\": label, \"image\": actual_image})   \n",
    "                    \n",
    "    random.shuffle(features_with_labels)\n",
    "    #print(X[0][0])\n",
    "    #print(X[0][1])\n",
    "    X = [x[\"feature\"] for x in features_with_labels]\n",
    "    y = [x[\"label\"] for x in features_with_labels]\n",
    "    images = [x[\"image\"] for x in features_with_labels]\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X = X.astype('float32')\n",
    "    X /= 255\n",
    "\n",
    "    return X, np.array(y), images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5838, 70, 70, 1)\n",
      "(5838,)\n",
      "(291, 70, 70, 1)\n",
      "(291,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, _ = get_features_labels(\"/home/anurag/AR/data/23March/train\")\n",
    "X_test, y_test, test_images = get_features_labels(\"/home/anurag/AR/data/23March/test\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 150\n",
    "\n",
    "required_input_shape = (*IMAGE_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = \"fine_tune/models\"\n",
    "tensorboard_logs_folder_location = \"fine_tune/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_6_class_cnn_model(model_weights_location):\n",
    "    \"\"\"\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', input_shape=required_input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(2048))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # load the model weights\n",
    "    model.load_weights(model_weights_location)\n",
    "                           \n",
    "    adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 68, 68, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 68, 68, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 68, 68, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 34, 34, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              12847104  \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 1542      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 13,475,974\n",
      "Trainable params: 13,470,918\n",
      "Non-trainable params: 5,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_model = load_6_class_cnn_model(\"/home/anurag/AR/Extending-Board-Games-using-deep-learning/API/models/6_class_cnn.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4774 samples, validate on 60 samples\n",
      "Epoch 1/150\n",
      "4774/4774 [==============================] - 4s 736us/step - loss: 3.0724 - acc: 0.1850 - val_loss: 2.0603 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.33333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 2/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 2.4861 - acc: 0.2220 - val_loss: 1.7797 - val_acc: 0.3500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.33333 to 0.35000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 3/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 2.1722 - acc: 0.2694 - val_loss: 1.5848 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.35000 to 0.41667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 4/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 1.9598 - acc: 0.3238 - val_loss: 1.4726 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.41667\n",
      "Epoch 5/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 1.8081 - acc: 0.3666 - val_loss: 1.3914 - val_acc: 0.4833\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.41667 to 0.48333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 6/150\n",
      "4774/4774 [==============================] - ETA: 0s - loss: 1.6707 - acc: 0.402 - 2s 366us/step - loss: 1.6705 - acc: 0.4026 - val_loss: 1.3139 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.48333 to 0.50000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 7/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 1.5519 - acc: 0.4439 - val_loss: 1.2535 - val_acc: 0.5167\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.50000 to 0.51667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 8/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 1.4725 - acc: 0.4619 - val_loss: 1.2057 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.51667 to 0.53333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 9/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 1.3852 - acc: 0.4927 - val_loss: 1.1678 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.53333 to 0.55000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 10/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 1.3244 - acc: 0.5197 - val_loss: 1.1256 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.55000 to 0.56667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 11/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 1.2913 - acc: 0.5329 - val_loss: 1.0871 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.56667 to 0.58333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 12/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 1.2017 - acc: 0.5610 - val_loss: 1.0553 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.58333\n",
      "Epoch 13/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 1.1880 - acc: 0.5633 - val_loss: 1.0239 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.58333\n",
      "Epoch 14/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 1.1205 - acc: 0.5907 - val_loss: 0.9841 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.58333 to 0.61667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 15/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 1.0958 - acc: 0.5957 - val_loss: 0.9456 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.61667\n",
      "Epoch 16/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 1.0667 - acc: 0.6091 - val_loss: 0.9029 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.61667\n",
      "Epoch 17/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 1.0114 - acc: 0.6341 - val_loss: 0.8854 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.61667\n",
      "Epoch 18/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.9882 - acc: 0.6464 - val_loss: 0.8527 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.61667\n",
      "Epoch 19/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.9879 - acc: 0.6464 - val_loss: 0.8321 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.61667 to 0.63333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 20/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.9392 - acc: 0.6581 - val_loss: 0.8166 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.63333\n",
      "Epoch 21/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.8987 - acc: 0.6730 - val_loss: 0.7887 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.63333 to 0.65000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 22/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.8927 - acc: 0.6697 - val_loss: 0.7613 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.65000 to 0.66667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 23/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.8554 - acc: 0.6977 - val_loss: 0.7286 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.66667 to 0.70000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 24/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.8384 - acc: 0.7030 - val_loss: 0.7124 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.70000\n",
      "Epoch 25/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.8270 - acc: 0.7049 - val_loss: 0.6905 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.70000\n",
      "Epoch 26/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.7849 - acc: 0.7103 - val_loss: 0.6786 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.70000\n",
      "Epoch 27/150\n",
      "4774/4774 [==============================] - 2s 370us/step - loss: 0.7776 - acc: 0.7162 - val_loss: 0.6556 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.70000 to 0.73333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 28/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.7553 - acc: 0.7331 - val_loss: 0.6345 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.73333 to 0.78333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 29/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.7171 - acc: 0.7449 - val_loss: 0.6244 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.78333\n",
      "Epoch 30/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.7275 - acc: 0.7463 - val_loss: 0.6150 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.78333\n",
      "Epoch 31/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.7137 - acc: 0.7472 - val_loss: 0.5951 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.78333\n",
      "Epoch 32/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.7039 - acc: 0.7444 - val_loss: 0.5852 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.78333\n",
      "Epoch 33/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.6804 - acc: 0.7545 - val_loss: 0.5778 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.78333\n",
      "Epoch 34/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.6506 - acc: 0.7658 - val_loss: 0.5602 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.78333 to 0.80000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 35/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.6386 - acc: 0.7656 - val_loss: 0.5365 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.80000 to 0.81667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 36/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.6266 - acc: 0.7780 - val_loss: 0.5379 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81667\n",
      "Epoch 37/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.5989 - acc: 0.7953 - val_loss: 0.5332 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81667\n",
      "Epoch 38/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.6061 - acc: 0.7910 - val_loss: 0.5057 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.81667 to 0.83333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 39/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.5957 - acc: 0.7880 - val_loss: 0.4969 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83333\n",
      "Epoch 40/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 0.5654 - acc: 0.7989 - val_loss: 0.4868 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.83333 to 0.85000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 41/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.5566 - acc: 0.8069 - val_loss: 0.4730 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85000\n",
      "Epoch 42/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.5464 - acc: 0.8102 - val_loss: 0.4650 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85000\n",
      "Epoch 43/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.5548 - acc: 0.8044 - val_loss: 0.4485 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85000\n",
      "Epoch 44/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.5141 - acc: 0.8226 - val_loss: 0.4469 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.85000\n",
      "Epoch 45/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.5130 - acc: 0.8255 - val_loss: 0.4328 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.85000\n",
      "Epoch 46/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.5156 - acc: 0.8161 - val_loss: 0.4247 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.85000\n",
      "Epoch 47/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.4891 - acc: 0.8301 - val_loss: 0.4089 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.85000 to 0.86667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 48/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.4738 - acc: 0.8375 - val_loss: 0.4036 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86667\n",
      "Epoch 49/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.4675 - acc: 0.8393 - val_loss: 0.3902 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.86667 to 0.88333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 50/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.4593 - acc: 0.8385 - val_loss: 0.3820 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88333\n",
      "Epoch 51/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.4523 - acc: 0.8423 - val_loss: 0.3772 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88333\n",
      "Epoch 52/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.4471 - acc: 0.8454 - val_loss: 0.3622 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88333\n",
      "Epoch 53/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.4402 - acc: 0.8506 - val_loss: 0.3528 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88333\n",
      "Epoch 54/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.4257 - acc: 0.8550 - val_loss: 0.3542 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88333\n",
      "Epoch 55/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.4258 - acc: 0.8527 - val_loss: 0.3452 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.88333 to 0.90000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 56/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.4164 - acc: 0.8523 - val_loss: 0.3328 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90000\n",
      "Epoch 57/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.4094 - acc: 0.8622 - val_loss: 0.3354 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90000\n",
      "Epoch 58/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.3897 - acc: 0.8722 - val_loss: 0.3241 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90000\n",
      "Epoch 59/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.3999 - acc: 0.8636 - val_loss: 0.3142 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90000\n",
      "Epoch 60/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.4030 - acc: 0.8651 - val_loss: 0.3070 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90000\n",
      "Epoch 61/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.3855 - acc: 0.8691 - val_loss: 0.3010 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90000\n",
      "Epoch 62/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.3802 - acc: 0.8703 - val_loss: 0.3028 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90000\n",
      "Epoch 63/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.3765 - acc: 0.8768 - val_loss: 0.2949 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90000\n",
      "Epoch 64/150\n",
      "4774/4774 [==============================] - ETA: 0s - loss: 0.3519 - acc: 0.883 - 2s 366us/step - loss: 0.3548 - acc: 0.8817 - val_loss: 0.2861 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90000\n",
      "Epoch 65/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.3469 - acc: 0.8840 - val_loss: 0.2846 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90000\n",
      "Epoch 66/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.3526 - acc: 0.8835 - val_loss: 0.2751 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.90000 to 0.93333, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 67/150\n",
      "4774/4774 [==============================] - 2s 371us/step - loss: 0.3498 - acc: 0.8863 - val_loss: 0.2722 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.93333\n",
      "Epoch 68/150\n",
      "4774/4774 [==============================] - 2s 367us/step - loss: 0.3332 - acc: 0.8940 - val_loss: 0.2708 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.93333\n",
      "Epoch 69/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.3384 - acc: 0.8846 - val_loss: 0.2592 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.93333\n",
      "Epoch 70/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.3258 - acc: 0.8961 - val_loss: 0.2525 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.93333\n",
      "Epoch 71/150\n",
      "4774/4774 [==============================] - 2s 368us/step - loss: 0.3195 - acc: 0.8923 - val_loss: 0.2578 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.93333\n",
      "Epoch 72/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.3199 - acc: 0.8944 - val_loss: 0.2436 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.93333\n",
      "Epoch 73/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.2978 - acc: 0.9022 - val_loss: 0.2388 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.93333\n",
      "Epoch 74/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.2981 - acc: 0.9032 - val_loss: 0.2385 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.93333\n",
      "Epoch 75/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.2880 - acc: 0.9066 - val_loss: 0.2338 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.93333\n",
      "Epoch 76/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.2980 - acc: 0.9064 - val_loss: 0.2177 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.93333\n",
      "Epoch 77/150\n",
      "4774/4774 [==============================] - 2s 369us/step - loss: 0.2884 - acc: 0.9066 - val_loss: 0.2213 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.93333\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4774/4774 [==============================] - 2s 362us/step - loss: 0.2813 - acc: 0.9118 - val_loss: 0.2210 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.93333\n",
      "Epoch 79/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.2661 - acc: 0.9154 - val_loss: 0.2119 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.93333\n",
      "Epoch 80/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 0.2641 - acc: 0.9160 - val_loss: 0.2143 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.93333\n",
      "Epoch 81/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2756 - acc: 0.9093 - val_loss: 0.2012 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.93333 to 0.95000, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 82/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.2574 - acc: 0.9177 - val_loss: 0.1959 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.95000\n",
      "Epoch 83/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2636 - acc: 0.9156 - val_loss: 0.1960 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.95000\n",
      "Epoch 84/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 0.2576 - acc: 0.9168 - val_loss: 0.1917 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.95000 to 0.96667, saving model to fine_tune/models\\6_class_cnn_fine_tuned.hdf5\n",
      "Epoch 85/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.2423 - acc: 0.9284 - val_loss: 0.1883 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.96667\n",
      "Epoch 86/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 0.2428 - acc: 0.9231 - val_loss: 0.1840 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.96667\n",
      "Epoch 87/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2457 - acc: 0.9225 - val_loss: 0.1811 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.96667\n",
      "Epoch 88/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2384 - acc: 0.9248 - val_loss: 0.1727 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.96667\n",
      "Epoch 89/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2312 - acc: 0.9323 - val_loss: 0.1743 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.96667\n",
      "Epoch 90/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2276 - acc: 0.9302 - val_loss: 0.1719 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.96667\n",
      "Epoch 91/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2333 - acc: 0.9296 - val_loss: 0.1738 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.96667\n",
      "Epoch 92/150\n",
      "4774/4774 [==============================] - 2s 362us/step - loss: 0.2221 - acc: 0.9286 - val_loss: 0.1732 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.96667\n",
      "Epoch 93/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2279 - acc: 0.9328 - val_loss: 0.1717 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.96667\n",
      "Epoch 94/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.2122 - acc: 0.9328 - val_loss: 0.1594 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.96667\n",
      "Epoch 95/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.2083 - acc: 0.9399 - val_loss: 0.1583 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.96667\n",
      "Epoch 96/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.2092 - acc: 0.9349 - val_loss: 0.1531 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.96667\n",
      "Epoch 97/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.1968 - acc: 0.9407 - val_loss: 0.1602 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.96667\n",
      "Epoch 98/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.2095 - acc: 0.9338 - val_loss: 0.1469 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.96667\n",
      "Epoch 99/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.1902 - acc: 0.9453 - val_loss: 0.1396 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.96667\n",
      "Epoch 100/150\n",
      "4774/4774 [==============================] - 2s 363us/step - loss: 0.1959 - acc: 0.9424 - val_loss: 0.1356 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.96667\n",
      "Epoch 101/150\n",
      "4774/4774 [==============================] - 2s 366us/step - loss: 0.1994 - acc: 0.9397 - val_loss: 0.1461 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.96667\n",
      "Epoch 102/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.1848 - acc: 0.9489 - val_loss: 0.1420 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.96667\n",
      "Epoch 103/150\n",
      "4774/4774 [==============================] - 2s 365us/step - loss: 0.1817 - acc: 0.9426 - val_loss: 0.1383 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.96667\n",
      "Epoch 104/150\n",
      "4774/4774 [==============================] - 2s 364us/step - loss: 0.1802 - acc: 0.9466 - val_loss: 0.1422 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.96667\n",
      "Epoch 00104: early stopping\n",
      "Test score: 0.14222583870093028\n",
      "Test accuracy: 0.9666666746139526\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "if not os.path.exists(model_folder_name):\n",
    "    os.makedirs(model_folder_name)\n",
    "\n",
    "# tensorboard logs\n",
    "if not os.path.exists(tensorboard_logs_folder_location):\n",
    "    os.makedirs(tensorboard_logs_folder_location)\n",
    "\n",
    "filepath = os.path.join(model_folder_name, \"6_class_cnn_fine_tuned.hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=20, verbose=1, mode='max')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_logs_folder_location, histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop, tensorboard]\n",
    "\n",
    "adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "trained_model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "hist = trained_model.fit(X_train, y_train, shuffle=True, batch_size=batch_size,\n",
    "                 epochs=epochs, verbose=1,\n",
    "                 validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "score, accuracy = trained_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper method to print a confusion matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [100, 15]\n",
    "\n",
    "IMAGES_PER_ROW = 5\n",
    "\n",
    "def plot_mispredicted_images(images, actual_values, predicted_values, mapping_func = None):\n",
    "    print(actual_values)\n",
    "    mis_predictions = actual_values ^ predicted_values\n",
    "    mis_prediction_indices = np.nonzero(mis_predictions)[0]\n",
    "    print(mis_prediction_indices)\n",
    "    \n",
    "    num_failed_images = len(mis_prediction_indices)\n",
    "    \n",
    "    if num_failed_images == 0:\n",
    "        print(\"All images predicted correctly.\")\n",
    "        return\n",
    "        \n",
    "    \n",
    "    if num_failed_images == 1:\n",
    "        print(\"{0} mispredicted as {1}\".format(actual_values[mis_prediction_indices[0]], predicted_values[mis_prediction_indices[0]]))\n",
    "        plt.imshow(images[mis_prediction_indices[0]], cmap='gray')\n",
    "        return\n",
    "    elif num_failed_images < IMAGES_PER_ROW:\n",
    "        num_images_per_row = 2\n",
    "    else:\n",
    "        num_images_per_row = IMAGES_PER_ROW\n",
    "        \n",
    "    num_rows = (num_failed_images // num_images_per_row) + int((num_failed_images % num_images_per_row) != 0)\n",
    "    if num_rows == 1:\n",
    "        num_rows = 2\n",
    "\n",
    "    print(\"Number of failed images: \" + str(num_failed_images))\n",
    "    print(\"Num rows: {0}. Num images/row: {1}\".format(num_rows, num_images_per_row))\n",
    "    #print(num_rows)\n",
    "    #print(num_images_per_row)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_images_per_row)\n",
    "\n",
    "    current_image_idx = 0\n",
    "\n",
    "    for itr in range(num_rows):\n",
    "        #print(itr)\n",
    "        for jtr in range(num_images_per_row):\n",
    "            if current_image_idx == num_failed_images:\n",
    "                break\n",
    "            \n",
    "            print(\"{0}, {1}, {2}\".format(itr, jtr, current_image_idx))\n",
    "            axes[itr, jtr].imshow(images[mis_prediction_indices[current_image_idx]], cmap='gray')\n",
    "            if mapping_func:\n",
    "                axes[itr, jtr].set_title(\"{0} predicted as {1}\".format(mapping_func(actual_values[mis_prediction_indices[current_image_idx]]), mapping_func(predicted_values[mis_prediction_indices[current_image_idx]])))\n",
    "            else:\n",
    "                axes[itr, jtr].set_title(\"{0} predicted as {1}\".format(actual_values[mis_prediction_indices[current_image_idx]], predicted_values[mis_prediction_indices[current_image_idx]]))\n",
    "            #print(current_image_idx)\n",
    "            current_image_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predictions using the best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b1_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b11_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b5_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b6_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b13_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b15_g_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b11_f_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b10_f_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b7_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b5_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b14_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b3_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b13_g_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b2_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b4_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b10_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b9_f_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b8_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b12_g_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b9_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b7_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b14_g_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b2_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b12_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b4_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b3_e_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b1_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b8_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b6_g_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bb/b15_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b3_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b7_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b12_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b9_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b15_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b11_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b10_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b3_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b10_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b13_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b14_a_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b14_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b9_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b2_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b7_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b4_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b4_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b13_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b11_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b1_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b12_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b8_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b5_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b6_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b15_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b5_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b1_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b8_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b6_b_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wb/b2_f_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b13_h_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b8_g_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b2_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b7_c_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b12_h_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b9_g_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b14_h_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b8_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b5_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b6_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b4_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b10_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b5_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b7_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b1_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b13_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b4_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b15_h_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b9_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b11_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b14_c_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b2_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b15_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b1_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b11_h_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b10_g_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b3_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b6_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b12_e_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bn/b3_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b11_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b12_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b2_c_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b3_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b14_d_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b9_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b10_b_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b6_c_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b10_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b4_c_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b7_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b7_b_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b12_b_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b13_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b8_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b14_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b15_f_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b13_d_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b11_b_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b5_c_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b2_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b15_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b6_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b5_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b1_c_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b1_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b4_d_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b3_c_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b8_b_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wn/b9_b_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b1_e_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b12_d_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b4_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b9_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b6_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b7_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b2_e_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b11_d_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b15_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b14_c_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b5_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b10_d_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b3_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b13_d_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bk/b8_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b12_g_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b8_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b3_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b11_g_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b7_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b15_h_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b9_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b6_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b1_e_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b4_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b14_g_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b2_e_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b5_g_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b13_g_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wk/b10_g_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b14_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b3_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b1_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b6_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b11_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b2_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b4_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b11_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b13_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b9_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b10_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b8_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b10_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b3_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b9_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b5_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b5_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b6_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b12_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b2_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b7_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b1_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b4_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b8_f_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b15_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bp/b7_c_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b5_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b13_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b9_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b10_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b4_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b9_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b2_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b8_e_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b6_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b6_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b7_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b1_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b12_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b1_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b11_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b8_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b2_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b14_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b3_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b3_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b10_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b15_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b5_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b7_f_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wp/b4_e_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b1_d_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b15_e_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b9_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b4_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b2_d_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b12_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b13_e_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b3_d_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b14_e_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b8_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b10_e_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b11_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b6_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b7_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/bq/b5_d_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b11_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b6_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b1_d_1.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b4_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b15_e_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b9_c_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b12_c_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b8_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b13_e_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b10_c_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b14_e_7.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b3_d_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b2_d_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b5_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wq/b7_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b10_h_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b13_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b15_d_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b11_h_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b6_h_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b4_a_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b12_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b7_a_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b3_a_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b2_h_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b14_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b8_h_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b3_h_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b9_h_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b12_h_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b11_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b14_d_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b9_a_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b2_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b5_a_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b1_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b6_a_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b7_h_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b1_h_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b10_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b8_a_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b13_d_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b15_b_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b4_h_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/br/b5_h_3.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b8_b_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b6_b_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b9_b_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b10_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b10_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b1_a_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b11_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b3_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b9_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b2_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b5_b_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b6_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b14_b_8.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b13_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b4_g_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b4_b_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b7_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b14_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b15_c_2.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b12_f_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b2_g_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b5_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b8_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b11_g_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b12_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b3_g_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b7_b_5.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b13_b_6.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b1_h_4.jpg\n",
      "/home/anurag/AR/sriraj_v2/test_v2/wr/b15_b_8.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 1, 0, 1, 3, 1, 0, 3, 5, 2, 4, 0, 1, 4, 0, 3, 0, 4, 5, 1, 1, 2,\n",
       "       0, 1, 2, 5, 3, 1, 5, 1, 4, 5, 4, 2, 2, 2, 4, 2, 3, 3, 0, 3, 3, 4,\n",
       "       1, 3, 5, 2, 3, 4, 3, 3, 4, 4, 5, 2, 1, 4, 5, 1, 3, 0, 0, 2, 3, 3,\n",
       "       1, 0, 2, 2, 3, 3, 1, 1, 4, 0, 2, 2, 3, 1, 3, 3, 0, 1, 0, 0, 1, 0,\n",
       "       2, 0, 1, 3, 4, 4, 3, 1, 2, 2, 1, 5, 4, 2, 0, 0, 4, 5, 3, 3, 0, 2,\n",
       "       2, 0, 0, 2, 5, 0, 1, 3, 5, 1, 2, 2, 1, 0, 2, 2, 3, 2, 3, 1, 2, 0,\n",
       "       3, 3, 5, 1, 5, 0, 3, 3, 4, 1, 3, 3, 3, 4, 2, 2, 1, 0, 4, 3, 1, 3,\n",
       "       1, 0, 5, 4, 5, 5, 2, 1, 0, 0, 3, 2, 1, 3, 3, 3, 1, 0, 2, 1, 1, 2,\n",
       "       2, 2, 2, 2, 0, 2, 3, 5, 3, 2, 0, 2, 3, 2, 0, 2, 2, 2, 4, 2, 2, 3,\n",
       "       0, 3, 3, 4, 2, 5, 2, 5, 1, 4, 1, 1, 2, 0, 0, 2, 0, 3, 5, 0, 0, 1,\n",
       "       4, 3, 4, 0, 5, 1, 0, 5, 2, 0, 5, 3, 1, 3, 5, 1, 0, 3, 1, 5, 1, 3,\n",
       "       1, 2, 4, 1, 3, 1, 5, 2, 0, 4, 1, 0, 4, 0, 4, 0, 1, 3, 1, 3, 2, 1,\n",
       "       1, 2, 0, 1, 5, 3, 0, 3, 1, 2, 3, 1, 2, 1, 2, 2, 1, 1, 3, 1, 1, 0,\n",
       "       4, 0, 5, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test, test_images = get_features_labels(\"/home/anurag/AR/sriraj_v2/test_v2\")\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 4.2773388960107495\n",
      "Test accuracy: 0.1958762887622073\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test data\n",
    "score, accuracy = trained_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[16  2  0 28  0  5]\n",
      " [37  1  0 16  0  6]\n",
      " [28  2  0 29  1  0]\n",
      " [22  0  1 32  3  2]\n",
      " [ 9  3  0 17  0  1]\n",
      " [14  0  1  7  0  8]]\n",
      "0.1958762886597938\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88381a73c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "class_names = [\"pawn\", \"bishop\", \"knight\", \"rook\", \"queen\", \"king\"]\n",
    "\n",
    "#trained_model_path = \"/home/anurag/AR/trained_models/23Mar/6_class_cnn_fine_tuned.hdf5\"\n",
    "#trained_model = load_6_class_cnn_model(trained_model_path)\n",
    "\n",
    "# trained_model.load_weights(filepath)\n",
    "\n",
    "test_predictions = trained_model.predict(X_test, batch_size=batch_size)\n",
    "y_test_pred = [np.argmax(x) for x in test_predictions]\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False,title='Normalized confusion matrix')\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_location = \"/home/anurag/AR/analysis_results/29Mar2\"\n",
    "def save_mispredicted_images(images, actual_values, predicted_values, mapping_func = None):\n",
    "    # print(actual_values)\n",
    "    mis_predictions = actual_values ^ predicted_values\n",
    "    mis_prediction_indices = np.nonzero(mis_predictions)[0]\n",
    "    print(mis_prediction_indices)\n",
    "    \n",
    "    num_failed_images = len(mis_prediction_indices)\n",
    "    \n",
    "    if num_failed_images == 0:\n",
    "        print(\"All images predicted correctly.\")\n",
    "        return\n",
    "        \n",
    "    \n",
    "    if num_failed_images == 1:\n",
    "        print(\"{0} mispredicted as {1}\".format(actual_values[mis_prediction_indices[0]], predicted_values[mis_prediction_indices[0]]))\n",
    "        plt.imshow(images[mis_prediction_indices[0]], cmap='gray')\n",
    "        return\n",
    "    elif num_failed_images < IMAGES_PER_ROW:\n",
    "        num_images_per_row = 2\n",
    "    else:\n",
    "        num_images_per_row = IMAGES_PER_ROW\n",
    "        \n",
    "    num_rows = (num_failed_images // num_images_per_row) + int((num_failed_images % num_images_per_row) != 0)\n",
    "    if num_rows == 1:\n",
    "        num_rows = 2\n",
    "\n",
    "    print(\"Number of failed images: \" + str(num_failed_images))\n",
    "    print(\"Num rows: {0}. Num images/row: {1}\".format(num_rows, num_images_per_row))\n",
    "    #print(num_rows)\n",
    "    #print(num_images_per_row)\n",
    "\n",
    "\n",
    "    # fig, axes = plt.subplots(num_rows, num_images_per_row)\n",
    "\n",
    "    current_image_idx = 0\n",
    "\n",
    "    for itr in range(num_rows):\n",
    "        #print(itr)\n",
    "        for jtr in range(num_images_per_row):\n",
    "            if current_image_idx == num_failed_images:\n",
    "                break\n",
    "            \n",
    "            # print(\"{0}, {1}, {2}\".format(itr, jtr, current_image_idx))\n",
    "            \n",
    "            # axes[itr, jtr].imshow(images[mis_prediction_indices[current_image_idx]], cmap='gray')\n",
    "            if mapping_func:\n",
    "                image_location = os.path.join(base_location, str(current_image_idx) + \"_\" + \"{0}_as_{1}.jpg\".format(mapping_func(actual_values[mis_prediction_indices[current_image_idx]]), mapping_func(predicted_values[mis_prediction_indices[current_image_idx]])))\n",
    "                cv2.imwrite(image_location, images[mis_prediction_indices[current_image_idx]])\n",
    "    \n",
    "            else:\n",
    "                image_location = os.path.join(base_location, str(current_image_idx) + \"_\" + \"{0}_as_{1}.jpg\".format(actual_values[mis_prediction_indices[current_image_idx]], predicted_values[mis_prediction_indices[current_image_idx]]))\n",
    "                cv2.imwrite(image_location, images[mis_prediction_indices[current_image_idx]])\n",
    "            #print(current_image_idx)\n",
    "            current_image_idx += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'p', 1: 'b', 2: 'n', 3: 'r', 4: 'q', 5: 'k'}\n",
      "[  0   1   2   3   5   6   8   9  10  15  16  17  18  20  21  23  24  25\n",
      "  26  28  30  31  33  34  35  36  37  39  40  44  45  46  47  48  49  52\n",
      "  53  54  58  59  60  61  63  66  68  69  71  73  75  76  77  79  80  81\n",
      "  83  86  87  88  89  90  91  92  93  94  95  96  97  99 100 101 102 104\n",
      " 105 106 108 109 110 112 113 114 116 119 120 121 122 123 124 126 127 129\n",
      " 131 132 133 134 136 138 139 140 141 142 144 145 147 148 149 150 151 152\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 170 172 175 176 177\n",
      " 178 179 181 182 183 184 185 186 187 191 192 193 196 197 201 202 203 204\n",
      " 205 208 209 210 211 213 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 231 233 234 235 237 238 240 242 243 244 245 246 247 248 249\n",
      " 252 253 254 255 257 258 260 262 263 265 266 267 268 270 271 272 273 276\n",
      " 277 278 279 280 281 282 283 284 285 286 287 288 290]\n",
      "Number of failed images: 211\n",
      "Num rows: 43. Num images/row: 5\n"
     ]
    }
   ],
   "source": [
    "# test_images = np.squeeze(X_test)\n",
    "reverse_label_mappings = {type_name_to_label[y]: y for y in type_name_to_label}\n",
    "print(reverse_label_mappings)\n",
    "save_mispredicted_images(test_images, y_test, y_test_pred, lambda x: reverse_label_mappings[x])\n",
    "# plot_mispredicted_images(test_images, y_test, y_test_pred, lambda x: reverse_label_mappings[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
