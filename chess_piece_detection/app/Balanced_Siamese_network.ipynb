{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Balanced Siamese network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ace-racer/Extending-Board-Games-using-deep-learning/blob/master/chess_piece_detection/app/Balanced_Siamese_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Umcq-GWbS4bX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initial code from: https://sorenbouma.github.io/blog/oneshot/\n",
        "\n",
        "\"\"\"\n",
        "import keras\n",
        "from keras.layers import Input, Conv2D, Lambda, average, Dense, Flatten,MaxPooling2D, BatchNormalization, Dropout, Activation, Subtract, subtract\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "import numpy.random as rng\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def W_init(shape,name=None):\n",
        "    \"\"\"Initialize weights as in paper\"\"\"\n",
        "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
        "    return K.variable(values,name=name)\n",
        "\n",
        "def b_init(shape,name=None):\n",
        "    \"\"\"Initialize bias as in paper\"\"\"\n",
        "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
        "    return K.variable(values,name=name)\n",
        "\n",
        "input_shape = (100, 100, 3)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "\n",
        "#build convnet to use in each siamese 'leg'\n",
        "convnet = Sequential()\n",
        "\n",
        "convnet.add(Conv2D(32,(5,5),input_shape=input_shape,\n",
        "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Activation('relu'))\n",
        "convnet.add(MaxPooling2D())\n",
        "\n",
        "convnet.add(Conv2D(64,(4,4), kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Activation('relu'))\n",
        "convnet.add(MaxPooling2D())\n",
        "\n",
        "convnet.add(Conv2D(128,(4,4), kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Activation('relu'))\n",
        "convnet.add(Flatten())\n",
        "convnet.add(Dropout(0.4))\n",
        "convnet.add(Dense(1024,activation=\"relu\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "\n",
        "#encode each of the two inputs into a vector with the convnet\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "\n",
        "#merge two encoded inputs with the average\n",
        "both = subtract([encoded_l,encoded_r])\n",
        "both = K.abs(both)\n",
        "# both = Dense(256, activation='relu')(both)\n",
        "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
        "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "\n",
        "\n",
        "optimizer = Adam(0.0005)\n",
        "\n",
        "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "print(siamese_net.count_params())\n",
        "print(siamese_net.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYgXCOqqW-Cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ace-racer/Chess-Pieces-Data.git\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2OVDeZtTTwk",
        "colab_type": "code",
        "outputId": "073946e2-c045-4205-ce07-573b7a616f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1679
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "import itertools\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import product, combinations\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "#samples_per_type = {\"b\": 30, \"n\": 25, \"k\": 25, \"p\": 35, \"q\": 25, \"r\": 35}\n",
        "samples_per_type = {\"b\": 3, \"n\": 2, \"k\": 2, \"p\": 3, \"q\": 2, \"r\": 3}\n",
        "\n",
        "# training parameters\n",
        "IMAGE_SIZE = (100, 100)\n",
        "CHECKPOINTS_LOCATION = \"weights\"\n",
        "LOGS_LOCATION = \"logs\"\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# change as required\n",
        "IMAGES_LOCATION = \"Chess-Pieces-Data/crawled_1901/\"\n",
        "#IMAGES_LOCATION = \"H:\\\\AR-ExtendingOnlineGames\\\\crawled_chess_piece_images\"\n",
        "\n",
        "if not os.path.exists(CHECKPOINTS_LOCATION):\n",
        "    os.makedirs(CHECKPOINTS_LOCATION)\n",
        "\n",
        "if not os.path.exists(LOGS_LOCATION):\n",
        "    os.makedirs(LOGS_LOCATION)\n",
        "\n",
        "def generate_paired_instances_by_ratio(folder_name, total_instances = 6000, different_records_ratio = 0.5):\n",
        "    \n",
        "    data = []\n",
        "    label_values = []\n",
        "    for type_name in samples_per_type:\n",
        "        piece_type_folder = os.path.join(folder_name, type_name)\n",
        "        for f in (os.listdir(piece_type_folder)):\n",
        "            img_file_loc = os.path.join(piece_type_folder, f)\n",
        "            data.append(img_file_loc)\n",
        "            label_values.append(type_name)\n",
        "    \n",
        "    num_categories = 6\n",
        "\n",
        "    # Get the counts of the individual labels\n",
        "    label_counts = Counter(label_values)\n",
        "    \n",
        "    # Get the label indices in the original data read from the file\n",
        "    label_indices = defaultdict(list)\n",
        "    for itr, val in enumerate(label_values):\n",
        "        label_indices[val].append(itr)\n",
        "    \n",
        "    num_same_items_per_category = int(math.ceil(np.sqrt((( 1- different_records_ratio ) * total_instances) / num_categories)))\n",
        "    num_different_items_per_category = int(math.ceil(np.sqrt((2 * different_records_ratio * total_instances)/(num_categories * (num_categories - 1)))))\n",
        "    print(\"Num same items per category: \" + str(num_same_items_per_category))\n",
        "    print(\"Num different items per category: \" + str(num_different_items_per_category))\n",
        "\n",
        "    most_common_categories = [x for x, _ in label_counts.most_common(num_categories)]\n",
        "    print(\"Most common categories...\")\n",
        "    print(most_common_categories)\n",
        "    \n",
        "    pairwise_indices_same_items = []\n",
        "    for label in most_common_categories:\n",
        "        required_indices = label_indices[label][:num_same_items_per_category]\n",
        "        similar_item_index_pairs = list(product(required_indices, required_indices))\n",
        "        pairwise_indices_same_items.extend(similar_item_index_pairs)\n",
        "\n",
        "    pairwise_indices_different_items = []\n",
        "    category_pairs = combinations(most_common_categories, 2)\n",
        "\n",
        "    for cat1, cat2 in category_pairs:\n",
        "        category1_indices = label_indices[cat1][:num_different_items_per_category]\n",
        "        category2_indices = label_indices[cat2][:num_different_items_per_category]\n",
        "        different_items_index_pairs = list(product(category1_indices, category2_indices))\n",
        "        pairwise_indices_different_items.extend(different_items_index_pairs)\n",
        "\n",
        "    print(\"Num same category pairs: \" + str(len(pairwise_indices_same_items)))\n",
        "    print(\"Num different category pairs: \" + str(len(pairwise_indices_different_items)))\n",
        "\n",
        "    instances_with_labels = []\n",
        "    for idx1, idx2 in pairwise_indices_same_items:\n",
        "        label = int(label_values[idx1] == label_values[idx2])\n",
        "        \n",
        "        img1 = cv2.imread(data[idx1])\n",
        "        img1 = cv2.resize(img1, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        img2 = cv2.imread(data[idx2])\n",
        "        img2 = cv2.resize(img2, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
        "        \n",
        "        instances_with_labels.append((img1, img2, label))\n",
        "\n",
        "    for idx1, idx2 in pairwise_indices_different_items:\n",
        "        label = int(label_values[idx1] == label_values[idx2])\n",
        "\n",
        "        img1 = cv2.imread(data[idx1])\n",
        "        img1 = cv2.resize(img1, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        img2 = cv2.imread(data[idx2])\n",
        "        img2 = cv2.resize(img2, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
        "        \n",
        "        instances_with_labels.append((img1, img2, label))\n",
        "\n",
        "    random.shuffle(instances_with_labels)\n",
        "    instances = np.array([[x[0], x[1]] for x in instances_with_labels])\n",
        "    labels = np.array([x[2] for x in instances_with_labels])\n",
        "\n",
        "    return instances, labels\n",
        "\n",
        "X_train_original = []\n",
        "y_train_original = []\n",
        "\n",
        "\n",
        "training_images = os.path.join(IMAGES_LOCATION, \"train\")\n",
        "\n",
        "X_train_original, y_train_original = generate_paired_instances_by_ratio(training_images)\n",
        "\n",
        "X_train_original = np.array(X_train_original)\n",
        "X_train_original = X_train_original.astype('float32')\n",
        "X_train_original /= 255\n",
        "y_train_original = np.array(y_train_original)\n",
        "\n",
        "print(X_train_original.shape)\n",
        "print(y_train_original.shape)\n",
        "\n",
        "# split into train and validation splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_original, y_train_original, test_size=0.25, random_state=42, stratify = y_train_original)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "X_train_left = X_train[:, 0, ...]\n",
        "X_train_right = X_train[:, 1, ...]\n",
        "print(X_train_left.shape)\n",
        "print(X_train_right.shape)\n",
        "\n",
        "X_test_left = X_test[:, 0, ...]\n",
        "X_test_right = X_test[:, 1, ...]\n",
        "\n",
        "\n",
        "filepath = os.path.join(CHECKPOINTS_LOCATION, \"siamese.hdf5\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=1, mode='max')\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=LOGS_LOCATION, histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "callbacks_list = [checkpoint, earlystop, tensorboard]\n",
        "\n",
        "\n",
        "model = siamese_net\n",
        "X_train_instances = [X_train_left, X_train_right]\n",
        "X_test_instances = [X_test_left, X_test_right]\n",
        "\n",
        "hist = model.fit(X_train_instances, y_train, shuffle=True, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS, verbose=1, validation_data=(X_test_instances, y_test), callbacks=callbacks_list)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num same items per category: 23\n",
            "Num different items per category: 15\n",
            "Most common categories...\n",
            "['r', 'b', 'n', 'k', 'p', 'q']\n",
            "Num same category pairs: 3174\n",
            "Num different category pairs: 3375\n",
            "(6549, 2, 100, 100, 3)\n",
            "(6549,)\n",
            "(4911, 2, 100, 100, 3)\n",
            "(1638, 2, 100, 100, 3)\n",
            "(4911,)\n",
            "(1638,)\n",
            "(4911, 100, 100, 3)\n",
            "(4911, 100, 100, 3)\n",
            "Train on 4911 samples, validate on 1638 samples\n",
            "Epoch 1/50\n",
            "4911/4911 [==============================] - 842s 172ms/step - loss: 9.7205 - acc: 0.5437 - val_loss: 7.4238 - val_acc: 0.5812\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58120, saving model to weights/siamese.hdf5\n",
            "Epoch 2/50\n",
            "4911/4911 [==============================] - 842s 171ms/step - loss: 3.3655 - acc: 0.6856 - val_loss: 2.4357 - val_acc: 0.7692\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.58120 to 0.76923, saving model to weights/siamese.hdf5\n",
            "Epoch 3/50\n",
            "4911/4911 [==============================] - 839s 171ms/step - loss: 2.1049 - acc: 0.7502 - val_loss: 1.7886 - val_acc: 0.7637\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.76923\n",
            "Epoch 4/50\n",
            "4911/4911 [==============================] - 834s 170ms/step - loss: 1.6230 - acc: 0.7705 - val_loss: 1.3773 - val_acc: 0.8071\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.76923 to 0.80708, saving model to weights/siamese.hdf5\n",
            "Epoch 5/50\n",
            "4911/4911 [==============================] - 836s 170ms/step - loss: 1.3534 - acc: 0.7878 - val_loss: 1.2049 - val_acc: 0.8156\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80708 to 0.81563, saving model to weights/siamese.hdf5\n",
            "Epoch 6/50\n",
            "4911/4911 [==============================] - 831s 169ms/step - loss: 1.2378 - acc: 0.7988 - val_loss: 1.1611 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.81563\n",
            "Epoch 7/50\n",
            "4911/4911 [==============================] - 836s 170ms/step - loss: 1.2021 - acc: 0.8068 - val_loss: 1.1014 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.81563 to 0.84554, saving model to weights/siamese.hdf5\n",
            "Epoch 8/50\n",
            "4911/4911 [==============================] - 835s 170ms/step - loss: 1.0264 - acc: 0.8408 - val_loss: 0.9930 - val_acc: 0.8394\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.84554\n",
            "Epoch 9/50\n",
            " 160/4911 [..............................] - ETA: 12:28 - loss: 0.9654 - acc: 0.8313"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7f35a4d2c7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mX_test_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_test_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "V8pn-JOU_9Os",
        "colab_type": "code",
        "outputId": "731dfce9-1ea5-446c-bb2c-ae67b151c1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chess-Pieces-Data  logs  sample_data  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZmpyyFTrAK3c",
        "colab_type": "code",
        "outputId": "5f1dfa5d-b3d0-49e2-c786-599073adb67c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r weights.zip weights\n",
        "!zip -r logs.zip logs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: weights/ (stored 0%)\n",
            "  adding: weights/siamese.hdf5 (deflated 6%)\n",
            "  adding: logs/ (stored 0%)\n",
            "  adding: logs/events.out.tfevents.1548678183.1ef9e7bc7e09 (deflated 8%)\n",
            "  adding: logs/events.out.tfevents.1548677857.1ef9e7bc7e09 (deflated 8%)\n",
            "  adding: logs/events.out.tfevents.1548678453.1ef9e7bc7e09 (deflated 8%)\n",
            "  adding: logs/events.out.tfevents.1548679544.1ef9e7bc7e09 (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X2bkUBEABGzY",
        "colab_type": "code",
        "outputId": "445dea3c-127f-40b1-859a-ec7f5bcd2ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chess-Pieces-Data  logs  logs.zip  sample_data\tweights  weights.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gq4i480CBJcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(\"weights.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Qm1I2R-BZs5",
        "colab_type": "code",
        "outputId": "eaab01e2-3064-44df-cbe8-5ec3e5890f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "files.download(\"logs.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 60082, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 721, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 800, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yjwkxS6Ep1wP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}