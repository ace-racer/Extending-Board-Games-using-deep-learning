{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47486401\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1024)         47485376    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 1024)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1025        subtract_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 47,486,401\n",
      "Trainable params: 47,485,953\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initial code from: https://sorenbouma.github.io/blog/oneshot/\n",
    "\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras.layers import Input, Conv2D, Lambda, average, Dense, Flatten,MaxPooling2D, BatchNormalization, Dropout, Activation, Subtract, subtract\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (100, 100, 3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "\n",
    "convnet.add(Conv2D(32,(5,5),input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Activation('relu'))\n",
    "convnet.add(MaxPooling2D())\n",
    "\n",
    "convnet.add(Conv2D(64,(4,4), kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Activation('relu'))\n",
    "convnet.add(MaxPooling2D())\n",
    "\n",
    "convnet.add(Conv2D(128,(4,4), kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Activation('relu'))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dropout(0.4))\n",
    "convnet.add(Dense(1024,activation=\"relu\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "\n",
    "#encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "#merge two encoded inputs with the average\n",
    "both = subtract([encoded_l,encoded_r])\n",
    "# both = K.abs(both)\n",
    "# both = Dense(256, activation='relu')(both)\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "\n",
    "optimizer = Adam(0.0005)\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(siamese_net.count_params())\n",
    "print(siamese_net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product, combinations\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "#samples_per_type = {\"b\": 30, \"n\": 25, \"k\": 25, \"p\": 35, \"q\": 25, \"r\": 35}\n",
    "samples_per_type = {\"b\": 3, \"n\": 2, \"k\": 2, \"p\": 3, \"q\": 2, \"r\": 3, \"empty\": 4}\n",
    "\n",
    "# training parameters\n",
    "IMAGE_SIZE = (70, 70)\n",
    "CHECKPOINTS_LOCATION = \"weights\"\n",
    "LOGS_LOCATION = \"logs\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# change as required\n",
    "IMAGES_LOCATION = \"H:\\\\AR-ExtendingOnlineGames\\\\data\\\\chess_pieces_data\\\\Chess ID Public Data\\\\train\"\n",
    "#IMAGES_LOCATION = \"H:\\\\AR-ExtendingOnlineGames\\\\crawled_chess_piece_images\"\n",
    "\n",
    "if not os.path.exists(CHECKPOINTS_LOCATION):\n",
    "    os.makedirs(CHECKPOINTS_LOCATION)\n",
    "\n",
    "if not os.path.exists(LOGS_LOCATION):\n",
    "    os.makedirs(LOGS_LOCATION)\n",
    "\n",
    "def generate_paired_instances_by_ratio(folder_name, total_instances = 6000, different_records_ratio = 0.5):\n",
    "    \n",
    "    data = []\n",
    "    label_values = []\n",
    "    for type_name in samples_per_type:\n",
    "        piece_type_folder = os.path.join(folder_name, type_name)\n",
    "        for f in (os.listdir(piece_type_folder)):\n",
    "            img_file_loc = os.path.join(piece_type_folder, f)\n",
    "            data.append(img_file_loc)\n",
    "            label_values.append(type_name)\n",
    "    \n",
    "    num_categories = 7\n",
    "\n",
    "    # Get the counts of the individual labels\n",
    "    label_counts = Counter(label_values)\n",
    "    \n",
    "    # Get the label indices in the original data read from the file\n",
    "    label_indices = defaultdict(list)\n",
    "    for itr, val in enumerate(label_values):\n",
    "        label_indices[val].append(itr)\n",
    "    \n",
    "    num_same_items_per_category = int(math.ceil(np.sqrt((( 1- different_records_ratio ) * total_instances) / num_categories)))\n",
    "    num_different_items_per_category = int(math.ceil(np.sqrt((2 * different_records_ratio * total_instances)/(num_categories * (num_categories - 1)))))\n",
    "    print(\"Num same items per category: \" + str(num_same_items_per_category))\n",
    "    print(\"Num different items per category: \" + str(num_different_items_per_category))\n",
    "\n",
    "    most_common_categories = [x for x, _ in label_counts.most_common(num_categories)]\n",
    "    print(\"Most common categories...\")\n",
    "    print(most_common_categories)\n",
    "    \n",
    "    pairwise_indices_same_items = []\n",
    "    for label in most_common_categories:\n",
    "        required_indices = label_indices[label][:num_same_items_per_category]\n",
    "        similar_item_index_pairs = list(product(required_indices, required_indices))\n",
    "        pairwise_indices_same_items.extend(similar_item_index_pairs)\n",
    "\n",
    "    pairwise_indices_different_items = []\n",
    "    category_pairs = combinations(most_common_categories, 2)\n",
    "\n",
    "    for cat1, cat2 in category_pairs:\n",
    "        category1_indices = label_indices[cat1][:num_different_items_per_category]\n",
    "        category2_indices = label_indices[cat2][:num_different_items_per_category]\n",
    "        different_items_index_pairs = list(product(category1_indices, category2_indices))\n",
    "        pairwise_indices_different_items.extend(different_items_index_pairs)\n",
    "\n",
    "    print(\"Num same category pairs: \" + str(len(pairwise_indices_same_items)))\n",
    "    print(\"Num different category pairs: \" + str(len(pairwise_indices_different_items)))\n",
    "\n",
    "    instances_with_labels = []\n",
    "    for idx1, idx2 in pairwise_indices_same_items:\n",
    "        label = int(label_values[idx1] == label_values[idx2])\n",
    "        \n",
    "        img1 = cv2.imread(data[idx1])\n",
    "        img1 = cv2.resize(img1, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        img2 = cv2.imread(data[idx2])\n",
    "        img2 = cv2.resize(img2, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        instances_with_labels.append((img1, img2, label))\n",
    "\n",
    "    for idx1, idx2 in pairwise_indices_different_items:\n",
    "        label = int(label_values[idx1] == label_values[idx2])\n",
    "\n",
    "        img1 = cv2.imread(data[idx1])\n",
    "        img1 = cv2.resize(img1, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        img2 = cv2.imread(data[idx2])\n",
    "        img2 = cv2.resize(img2, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        instances_with_labels.append((img1, img2, label))\n",
    "\n",
    "    random.shuffle(instances_with_labels)\n",
    "    instances = np.array([[x[0], x[1]] for x in instances_with_labels])\n",
    "    labels = np.array([x[2] for x in instances_with_labels])\n",
    "\n",
    "    return instances, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_original = []\n",
    "y_train_original = []\n",
    "\n",
    "\n",
    "training_images = os.path.join(IMAGES_LOCATION, \"train\")\n",
    "\n",
    "X_train_original, y_train_original = generate_paired_instances_by_ratio(training_images)\n",
    "\n",
    "X_train_original = np.array(X_train_original)\n",
    "X_train_original = X_train_original.astype('float32')\n",
    "X_train_original /= 255\n",
    "y_train_original = np.array(y_train_original)\n",
    "\n",
    "print(X_train_original.shape)\n",
    "print(y_train_original.shape)\n",
    "\n",
    "# split into train and validation splits\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_original, y_train_original, test_size=0.25, random_state=42, stratify = y_train_original)\n",
    "\n",
    "X_train = X_train_original\n",
    "y_train = y_train_original\n",
    "\n",
    "test_images = os.path.join(IMAGES_LOCATION, \"test\")\n",
    "X_test, y_test = generate_paired_instances_by_ratio(test_images)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train_left = X_train[:, 0, ...]\n",
    "X_train_right = X_train[:, 1, ...]\n",
    "print(X_train_left.shape)\n",
    "print(X_train_right.shape)\n",
    "\n",
    "X_test_left = X_test[:, 0, ...]\n",
    "X_test_right = X_test[:, 1, ...]\n",
    "\n",
    "\n",
    "filepath = os.path.join(CHECKPOINTS_LOCATION, \"siamese_7_class.hdf5\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=1, mode='max')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=LOGS_LOCATION, histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop, tensorboard]\n",
    "\n",
    "\n",
    "model = siamese_net\n",
    "X_train_instances = [X_train_left, X_train_right]\n",
    "X_test_instances = [X_test_left, X_test_right]\n",
    "\n",
    "hist = model.fit(X_train_instances, y_train, shuffle=True, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS, verbose=1, validation_data=(X_test_instances, y_test), callbacks=callbacks_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
