{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ace-racer/Extending-Board-Games-using-deep-learning/blob/master/chess_piece_detection/Siamese_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Umcq-GWbS4bX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1de523e2-90aa-4c43-b9c1-a961bb73e0ec"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initial code from: https://sorenbouma.github.io/blog/oneshot/\n",
        "\n",
        "\"\"\"\n",
        "import keras\n",
        "from keras.layers import Input, Conv2D, Lambda, average, Dense, Flatten,MaxPooling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "import numpy.random as rng\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def W_init(shape,name=None):\n",
        "    \"\"\"Initialize weights as in paper\"\"\"\n",
        "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
        "    return K.variable(values,name=name)\n",
        "\n",
        "def b_init(shape,name=None):\n",
        "    \"\"\"Initialize bias as in paper\"\"\"\n",
        "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
        "    return K.variable(values,name=name)\n",
        "\n",
        "input_shape = (100, 100, 3)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "\n",
        "#build convnet to use in each siamese 'leg'\n",
        "convnet = Sequential()\n",
        "convnet.add(Conv2D(32,(5,5),activation='relu',input_shape=input_shape,\n",
        "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(64,(4,4),activation='relu',\n",
        "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
        "convnet.add(Flatten())\n",
        "convnet.add(Dense(1024,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "\n",
        "#encode each of the two inputs into a vector with the convnet\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "\n",
        "#merge two encoded inputs with the average\n",
        "both = average([encoded_l,encoded_r])\n",
        "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
        "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
        "\n",
        "optimizer = Adam(0.00006)\n",
        "\n",
        "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "print(siamese_net.count_params())\n",
        "print(siamese_net.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47485505\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 1024)         47484480    input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "average_2 (Average)             (None, 1024)         0           sequential_2[1][0]               \n",
            "                                                                 sequential_2[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            1025        average_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 47,485,505\n",
            "Trainable params: 47,485,505\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NYgXCOqqW-Cs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "369bdc16-1bbb-407f-cb2f-c31b6125c5d9"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ace-racer/Chess-Pieces-Data.git\n",
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Chess-Pieces-Data' already exists and is not an empty directory.\n",
            "Chess-Pieces-Data  logs  sample_data  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K2OVDeZtTTwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "import itertools\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "# change as required\n",
        "IMAGES_LOCATION = \"Chess-Pieces-Data/crawled_1901/\"\n",
        "\n",
        "samples_per_type = {\"b\": 30, \"n\": 25, \"k\": 25, \"p\": 35, \"q\": 25, \"r\": 35}\n",
        "#samples_per_type = {\"b\": 3, \"n\": 2, \"k\": 2, \"p\": 3, \"q\": 2, \"r\": 3}\n",
        "\n",
        "# training parameters\n",
        "IMAGE_SIZE = (100, 100)\n",
        "CHECKPOINTS_LOCATION = \"weights\"\n",
        "LOGS_LOCATION = \"logs\"\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "if not os.path.exists(CHECKPOINTS_LOCATION):\n",
        "    os.makedirs(CHECKPOINTS_LOCATION)\n",
        "\n",
        "if not os.path.exists(LOGS_LOCATION):\n",
        "    os.makedirs(LOGS_LOCATION)\n",
        "\n",
        "X_train_original = []\n",
        "y_train_original = []\n",
        "\n",
        "\n",
        "training_images = os.path.join(IMAGES_LOCATION, \"train\")\n",
        "validation_images = os.path.join(IMAGES_LOCATION, \"test\")\n",
        "\n",
        "\n",
        "files_with_labels = []\n",
        "\n",
        "for type_name in samples_per_type:\n",
        "    piece_type_folder = os.path.join(training_images, type_name)\n",
        "    for idx, f in enumerate(os.listdir(piece_type_folder)):\n",
        "        if idx >= samples_per_type[type_name]:\n",
        "            break\n",
        "\n",
        "        img_file_loc = os.path.join(piece_type_folder, f)\n",
        "        files_with_labels.append((img_file_loc, type_name))\n",
        "\n",
        "\n",
        "random.shuffle(files_with_labels)\n",
        "# print(files_with_labels)\n",
        "\n",
        "cartesian_product = itertools.product(files_with_labels, files_with_labels)\n",
        "# print(cartesian_product)\n",
        "\n",
        "for item1, item2 in cartesian_product:\n",
        "\n",
        "    img1 = cv2.imread(item1[0])\n",
        "    img1 = cv2.resize(img1, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img2 = cv2.imread(item2[0])\n",
        "    img2 = cv2.resize(img2, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    label = int(item1[1] == item2[1])\n",
        "    X_train_original.append(np.array([img1, img2]))\n",
        "    y_train_original.append(label)\n",
        "\n",
        "X_train_original = np.array(X_train_original)\n",
        "plt.imshow(X_train_original[0][0])\n",
        "plt.imshow(X_train_original[0][1])\n",
        "\n",
        "X_train_original = X_train_original.astype('float32')\n",
        "X_train_original /= 255\n",
        "\n",
        "plt.imshow(X_train_original[0][0])\n",
        "plt.imshow(X_train_original[0][1])\n",
        "\n",
        "y_train_original = np.array(y_train_original)\n",
        "\n",
        "print(X_train_original.shape)\n",
        "print(y_train_original.shape)\n",
        "\n",
        "# split into train and validation splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_original, y_train_original, test_size=0.25, random_state=42, stratify = y_train_original)\n",
        "\n",
        "#X_train = np.array(X_train)\n",
        "#X_test = np.array(X_test)\n",
        "#y_train = np.array(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "X_train_left = X_train[:, 0, ...]\n",
        "X_train_right = X_train[:, 1, ...]\n",
        "print(X_train_left.shape)\n",
        "print(X_train_right.shape)\n",
        "\n",
        "X_test_left = X_test[:, 0, ...]\n",
        "X_test_right = X_test[:, 1, ...]\n",
        "\n",
        "filepath = os.path.join(CHECKPOINTS_LOCATION, \"siamese.hdf5\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=1, mode='max')\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=LOGS_LOCATION, histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "callbacks_list = [checkpoint, earlystop, tensorboard]\n",
        "\n",
        "X_train_instances = [X_train_left, X_train_right]\n",
        "# X_test_instances = [X_test_left, X_test_right]\n",
        "hist = siamese_net.fit(X_train_instances, y_train, shuffle=True, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS, verbose=1, validation_split = 0.25, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8pn-JOU_9Os",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZmpyyFTrAK3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r weights.zip weights\n",
        "!zip -r logs.zip logs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2bkUBEABGzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gq4i480CBJcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(\"weights.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Qm1I2R-BZs5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(\"logs.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}